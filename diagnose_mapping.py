#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–∞–ø–ø—ñ–Ω–≥—ñ–≤ —Ç–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É.

Author: Vladyslav V. Prodan
Contact: github.com/click0
Phone: +38(099)6053340
Version: 2.2.14
License: BSD 3-Clause "New" or "Revised" License
Year: 2025

–¶–µ–π —Å–∫—Ä–∏–ø—Ç —î —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –∫–æ–Ω—Ç—Ä–æ–ª—é —è–∫–æ—Å—Ç—ñ (QA) –¥–ª—è —Å–∏—Å—Ç–µ–º–∏ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è.
–í—ñ–Ω –≤–∏–∫–æ–Ω—É—î —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó:

1. üîé –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (Verify):
   –ü–æ—Ä—ñ–≤–Ω—é—î –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª (input.txt) –∑ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–º (input_recovery.txt).
   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç—Ä–∏—Ä—ñ–≤–Ω–µ–≤—É –ª–æ–≥—ñ–∫—É –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏, —â–æ–± —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –∑–º—ñ–Ω–∏ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è,
   –≤–∏–∫–ª–∏–∫–∞–Ω—ñ —Å–∫–ª–µ—é–≤–∞–Ω–Ω—è–º —Ä–æ–∑—ñ—Ä–≤–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤.

2. ‚öñÔ∏è  –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–∞–ø–ø—ñ–Ω–≥—ñ–≤ (Diff):
   –ü–æ—Ä—ñ–≤–Ω—é—î –¥–≤–∞ —Ñ–∞–π–ª–∏ JSON –º–∞–ø–ø—ñ–Ω–≥—É –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è "–¥—Ä–µ–π—Ñ—É" (Drift Check).
   –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ
   (–æ–¥–Ω–∞–∫–æ–≤–∏–π –≤—Ö—ñ–¥ –∑–∞–≤–∂–¥–∏ –¥–∞—î –æ–¥–Ω–∞–∫–æ–≤—É –º–∞—Å–∫—É).

3. üìä –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑:
   –ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤–µ—Ä—Å—ñ—é —Ñ–æ—Ä–º–∞—Ç—É –º–∞–ø–ø—ñ–Ω–≥—É (v1, v2.0, v2.1) —Ç–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
"""

import json
import sys
import argparse
import difflib
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# ============================================================================
# ‚öôÔ∏è –ì–õ–û–ë–ê–õ–¨–ù–Ü –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –¢–ê –ö–û–ù–°–¢–ê–ù–¢–ò
# ============================================================================

# –°–ø–∏—Å–æ–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π, –¥–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏–º–µ —Ñ–∞–π–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
SEARCH_DIRS = [Path('.'), Path('output'), Path('result')]

# –ì–ª–∏–±–∏–Ω–∞ –ø–æ—à—É–∫—É —ñ—Å—Ç–æ—Ä—ñ—ó —Ñ–∞–π–ª—ñ–≤ (—Å–∫—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ñ–∞–π–ª—ñ–≤ —Å–∫–∞–Ω—É–≤–∞—Ç–∏)
HISTORY_SEARCH_LIMIT = 20

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó (—à–∏—Ä–∏–Ω–∞ —Ç–∞–±–ª–∏—Ü—å —Ç–∞ –ª—ñ–º—ñ—Ç–∏ –≤–∏–≤–æ–¥—É)
SEPARATOR_WIDTH = 96
DIFF_LINE_LIMIT = 50

# –§–æ—Ä–º–∞—Ç–∏ —Ä—è–¥–∫—ñ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü—å (f-strings templates)
FMT_FILE_HEADER = "{:<10} | {:<40} | {:<8} | {:<20}"
FMT_CAT_ROW     = "{:<30} | {:<12} | {:<12} | {:<10} | {:<15}"


# ============================================================================
# –ë–õ–û–ö 1: –†–û–ë–û–¢–ê –ó JSON MAPPING (–ê–Ω–∞–ª—ñ–∑ —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)
# ============================================================================

def find_latest_maps(n_files: int = 2) -> List[Path]:
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å N –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ñ–∞–π–ª—ñ–≤ masking_map_*.json —É —Ä–æ–±–æ—á–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è—Ö.

    Args:
        n_files: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É (—Ö–æ—á–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ).

    Returns:
        List[Path]: –°–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π –∑–∞ —á–∞—Å–æ–º –∑–º—ñ–Ω–∏ (–Ω–∞–π–Ω–æ–≤—ñ—à—ñ –ø–µ—Ä—à—ñ).
    """
    candidates = []
    for d in SEARCH_DIRS:
        if d.exists():
            candidates.extend(d.glob("masking_map_*.json"))
    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è: –Ω–æ–≤—ñ—à—ñ —Ñ–∞–π–ª–∏ (–±—ñ–ª—å—à–∏–π mtime) –π–¥—É—Ç—å –ø–µ—Ä—à–∏–º–∏
    candidates.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    return candidates


def load_json(path: Path) -> Optional[Dict]:
    """
    –ë–µ–∑–ø–µ—á–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è JSON —Ñ–∞–π–ª—É –∑ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫.
    """
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è JSON {path}: {e}")
        return None


def analyze_category_structure(category_name: str, items: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª—ñ–∑—É—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'rank' –∞–±–æ 'surname').

    –í–∏–∑–Ω–∞—á–∞—î –≤–µ—Ä—Å—ñ—é —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–∏—Ö:
    - v1: –ø—Ä–æ—Å—Ç–∞ –ø–∞—Ä–∞ "–∫–ª—é—á": "–∑–Ω–∞—á–µ–Ω–Ω—è"
    - v2.0: —Å–ª–æ–≤–Ω–∏–∫ "–∫–ª—é—á": {"masked_as": "...", "gender": "..."}
    - v2.1: —Å–ª–æ–≤–Ω–∏–∫ –∑ instance tracking {"instances": [1, 2]}

    Returns:
        Dict –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é —Ç–∞ –≤–µ—Ä—Å—ñ—î—é —Ñ–æ—Ä–º–∞—Ç—É.
    """
    if not items:
        return {"count": 0, "format": "empty", "has_instances": False}

    # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –≤—Å—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
    first_value = list(items.values())[0]

    if isinstance(first_value, str):
        return {
            "count": len(items),
            "format": "v1",
            "structure": "original -> string",
            "has_instances": False
        }
    elif isinstance(first_value, dict):
        has_instances = "instances" in first_value

        # –ó–±—ñ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª—ñ–∑—ñ–π (—Å–∫—ñ–ª—å–∫–∏ —Ä–∞–∑—ñ–≤ –º–∞—Å–∫–∞ –ø–æ–≤—Ç–æ—Ä—é—î—Ç—å—Å—è)
        all_instances = [len(v.get("instances", [])) for v in items.values() if isinstance(v, dict)]
        instance_stats = {
            "total_instances": sum(all_instances),
            "max_instances": max(all_instances) if all_instances else 0
        } if has_instances else {}

        return {
            "count": len(items),
            "format": "v2.1" if has_instances else "v2.0",
            "structure": "original -> dict",
            "has_instances": has_instances,
            "instance_stats": instance_stats
        }
    return {"count": len(items), "format": "unknown", "has_instances": False}


def compare_mappings(file1: Path, file2: Path) -> None:
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –¥–≤–∞ JSON —Ñ–∞–π–ª–∏ –º–∞–ø–ø—ñ–Ω–≥—É (Drift Check).

    –ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –∑–º—ñ–Ω–∏–ª–æ—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –º–∞—Å–∫–∏ –¥–ª—è –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –∫–ª—é—á—ñ–≤.
    –Ø–∫—â–æ '–Ü–≤–∞–Ω–æ–≤' —É —Ñ–∞–π–ª—ñ –ê —Å—Ç–∞–≤ '–ü–µ—Ç—Ä–µ–Ω–∫–æ', –∞ —É —Ñ–∞–π–ª—ñ –ë ‚Äî '–°–∏–¥–æ—Ä–µ–Ω–∫–æ',
    —Ü–µ –æ–∑–Ω–∞—á–∞—î –ø–æ—Ä—É—à–µ–Ω–Ω—è –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–æ—Å—Ç—ñ (—Ä—ñ–∑–Ω–∏–π seed –∞–±–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ö–µ—à—É–≤–∞–Ω–Ω—è).
    """
    print("=" * SEPARATOR_WIDTH)
    print("‚öñÔ∏è  –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –§–ê–ô–õ–Ü–í –ú–ê–ü–ü–Ü–ù–ì–£ (DIFF)")
    print("=" * SEPARATOR_WIDTH)

    data1 = load_json(file1)
    data2 = load_json(file2)

    if not data1 or not data2:
        return

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
    t1 = datetime.fromtimestamp(file1.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
    t2 = datetime.fromtimestamp(file2.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')

    # –í–∏–≤—ñ–¥ —Ç–∞–±–ª–∏—Ü—ñ —Ñ–∞–π–ª—ñ–≤
    print("\n" + FMT_FILE_HEADER.format('–§–∞–π–ª', '–®–ª—è—Ö', '–í–µ—Ä—Å—ñ—è', '–°—Ç–≤–æ—Ä–µ–Ω–æ'))
    print("-" * SEPARATOR_WIDTH)
    print(FMT_FILE_HEADER.format('A (New)', str(file1.name), data1.get('version','?'), t1))
    print(FMT_FILE_HEADER.format('B (Old)', str(file2.name), data2.get('version','?'), t2))

    map1 = data1.get("mappings", {})
    map2 = data2.get("mappings", {})
    all_cats = sorted(set(map1.keys()) | set(map2.keys()))

    # –í–∏–≤—ñ–¥ —Ç–∞–±–ª–∏—Ü—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    print(f"\n{'‚îÄ' * SEPARATOR_WIDTH}")
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–ê–¢–ï–ì–û–†–Ü–Ø–•")
    print(f"{'‚îÄ' * SEPARATOR_WIDTH}")
    print(FMT_CAT_ROW.format('–ö–∞—Ç–µ–≥–æ—Ä—ñ—è', 'A –ö—ñ–ª—å–∫—ñ—Å—Ç—å', 'B –ö—ñ–ª—å–∫—ñ—Å—Ç—å', '–†—ñ–∑–Ω–∏—Ü—è', '–ó–º—ñ–Ω–µ–Ω—ñ –º–∞—Å–∫–∏'))
    print("-" * SEPARATOR_WIDTH)

    total_drift = 0

    for cat in all_cats:
        items1 = map1.get(cat, {})
        items2 = map2.get(cat, {})

        count1 = len(items1)
        count2 = len(items2)
        diff = count1 - count2
        diff_str = f"+{diff}" if diff > 0 else str(diff)

        # --- DRIFT CHECK ---
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ —Å–ø—ñ–ª—å–Ω—ñ –∫–ª—é—á—ñ
        drift_count = 0
        common_keys = set(items1.keys()) & set(items2.keys())
        for key in common_keys:
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è v1 —Ç–∞ v2
            val1 = items1[key]["masked_as"] if isinstance(items1[key], dict) else items1[key]
            val2 = items2[key]["masked_as"] if isinstance(items2[key], dict) else items2[key]
            if val1 != val2:
                drift_count += 1

        drift_str = f"‚ö†Ô∏è {drift_count}" if drift_count > 0 else "‚úì 0"
        total_drift += drift_count

        print(FMT_CAT_ROW.format(cat, count1, count2, diff_str, drift_str))

    print(f"\n{'=' * SEPARATOR_WIDTH}")
    if total_drift > 0:
        print(f"‚ö†Ô∏è  –£–í–ê–ì–ê: –ó–Ω–∞–π–¥–µ–Ω–æ {total_drift} –≤–∏–ø–∞–¥–∫—ñ–≤ –∑–º—ñ–Ω–∏ –º–∞—Å–∫–∏ –¥–ª—è —Ç–∏—Ö —Å–∞–º–∏—Ö –¥–∞–Ω–∏—Ö!")
        print("   –¶–µ –æ–∑–Ω–∞—á–∞—î, —â–æ –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω—ñ—Å—Ç—å –ø–æ—Ä—É—à–µ–Ω–∞ (–∞–±–æ –∑–º—ñ–Ω–µ–Ω–æ seed/hash/–∞–ª–≥–æ—Ä–∏—Ç–º).")
    else:
        print("‚úÖ –ú–∞—Å–∫–∏ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ (–¥–ª—è —Å–ø—ñ–ª—å–Ω–∏—Ö –∫–ª—é—á—ñ–≤ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–¥–Ω–∞–∫–æ–≤—ñ).")


def diagnose_single_file(map_path: str) -> None:
    """
    –í–∏–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ–¥–∏–Ω —Ñ–∞–π–ª –º–∞–ø–ø—ñ–Ω–≥—É –±–µ–∑ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è, –∫–æ–ª–∏ –≤ —ñ—Å—Ç–æ—Ä—ñ—ó –∑–Ω–∞–π–¥–µ–Ω–æ –ª–∏—à–µ –æ–¥–∏–Ω —Ñ–∞–π–ª.
    """
    map_file = Path(map_path)
    if not map_file.exists():
        print(f"‚ùå –ü–û–ú–ò–õ–ö–ê: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {map_file}")
        return

    print("=" * 80)
    print("üìã –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê MASKING MAP")
    print("=" * 80)
    print(f"\nüìÇ –§–∞–π–ª: {map_file}")

    data = load_json(map_file)
    if not data: return

    print(f"üî¢ –í–µ—Ä—Å—ñ—è:      {data.get('version', 'N/A')}")
    print(f"üïê Timestamp:   {data.get('timestamp', 'N/A')}")

    mappings = data.get("mappings", {})
    print(f"\n{'‚îÄ' * 80}")
    print("üìÅ –ö–ê–¢–ï–ì–û–†–Ü–á")
    print(f"{'‚îÄ' * 80}")

    for category_name in sorted(mappings.keys()):
        items = mappings[category_name]
        analysis = analyze_category_structure(category_name, items)
        print(f"\n  üì¶ {category_name.upper()}")
        print(f"     –ï–ª–µ–º–µ–Ω—Ç—ñ–≤:  {analysis['count']}")
        print(f"     –§–æ—Ä–º–∞—Ç:     {analysis.get('format')}")
        if analysis.get("has_instances"):
            stats = analysis.get("instance_stats", {})
            print(f"     ‚úì Instance tracking: {stats.get('total_instances', 0)} –ø–æ—Å–∏–ª–∞–Ω—å")


# ============================================================================
# –ë–õ–û–ö 2: –í–ï–†–ò–§–Ü–ö–ê–¶–Ü–Ø –¢–ï–ö–°–¢–£ (Recovered vs Original)
# ============================================================================

def find_original_and_recovery() -> tuple[Optional[Path], Optional[Path]]:
    """
    –®—É–∫–∞—î –ø–∞—Ä—É —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏: input.txt —Ç–∞ –Ω–∞–π—Å–≤—ñ–∂—ñ—à–∏–π input_recovery_*.txt.
    """
    original = Path("input.txt")
    if not original.exists():
        original = Path("output/input.txt")

    recovery_candidates = []
    for d in SEARCH_DIRS:
        if d.exists():
            recovery_candidates.extend(d.glob("input_recovery_*.txt"))

    recovery_candidates.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    latest_recovery = recovery_candidates[0] if recovery_candidates else None

    return (original if original.exists() else None), latest_recovery


def verify_text_recovery(original_path: Path, recovery_path: Path, ignore_flags: bool = False) -> None:
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º 3 —Ä—ñ–≤–Ω—ñ–≤ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏.

    –†—ñ–≤–Ω—ñ:
    1. Strict: –§–∞–π–ª–∏ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ –±–∞–π—Ç-–≤-–±–∞–π—Ç.
    2. Normalized: –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤ —Ä—è–¥–∫—ñ–≤ (–∑–∞–º—ñ–Ω–∞ \n –Ω–∞ –ø—Ä–æ–±—ñ–ª) —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–±—ñ–ª—ñ–≤.
       –¶–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ, –±–æ —Å–∫—Ä–∏–ø—Ç –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –º–æ–∂–µ "—Å–∫–ª–µ—é–≤–∞—Ç–∏" —Ä–æ–∑—ñ—Ä–≤–∞–Ω—ñ –∑–≤–∞–Ω–Ω—è (Fix #20).
    3. Skeleton: –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è –≤—Å—å–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è (–≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤). –ü–µ—Ä–µ–≤—ñ—Ä—è—î –ª–∏—à–µ –¥–∞–Ω—ñ.

    Args:
        original_path: –®–ª—è—Ö –¥–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É.
        recovery_path: –®–ª—è—Ö –¥–æ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É.
        ignore_flags: –Ø–∫—â–æ True, "–º'—è–∫—ñ" –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ (Level 2, 3) –≤–≤–∞–∂–∞—é—Ç—å—Å—è —É—Å–ø—ñ—Ö–æ–º.
    """
    print("=" * SEPARATOR_WIDTH)
    print("üîé –í–ï–†–ò–§–Ü–ö–ê–¶–Ü–Ø –í–Ü–î–ù–û–í–õ–ï–ù–ù–Ø –¢–ï–ö–°–¢–£")
    print("=" * SEPARATOR_WIDTH)
    print(f"üìÑ –û—Ä–∏–≥—ñ–Ω–∞–ª:   {original_path}")
    print(f"üìÑ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ: {recovery_path}")
    print("-" * SEPARATOR_WIDTH)

    try:
        with open(original_path, 'r', encoding='utf-8') as f1, \
                open(recovery_path, 'r', encoding='utf-8') as f2:
            content_orig = f1.read()
            content_rec = f2.read()

            # –ü–µ—Ä–µ–º–æ—Ç—É—î–º–æ –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó diff
            f1.seek(0)
            f2.seek(0)
            lines_orig = f1.readlines()
            lines_rec = f2.readlines()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤: {e}")
        return

    # --- –†–Ü–í–ï–ù–¨ 1: Strict ---
    if content_orig == content_rec:
        print("\n‚úÖ [LEVEL 1] –§–∞–π–ª–∏ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ (Byte-to-byte match).")
        return

    print("\n‚ùå [LEVEL 1] –°—É–≤–æ—Ä–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π—à–ª–∞. –§–∞–π–ª–∏ –º–∞—é—Ç—å –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ.")

    # --- –†–Ü–í–ï–ù–¨ 2: Normalized Newlines ---
    # –õ–æ–≥—ñ–∫–∞: –ó–∞–º—ñ–Ω—é—î–º–æ –≤—Å—ñ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±—ñ–ª.
    # –ü–æ—Ç—ñ–º —Å—Ö–ª–æ–ø—É—î–º–æ –º–Ω–æ–∂–∏–Ω–Ω—ñ –ø—Ä–æ–±—ñ–ª–∏ –≤ –æ–¥–∏–Ω.
    orig_norm = re.sub(r'[\n\r]+', ' ', content_orig)
    rec_norm = re.sub(r'[\n\r]+', ' ', content_rec)

    orig_norm = re.sub(r'\s+', ' ', orig_norm).strip()
    rec_norm = re.sub(r'\s+', ' ', rec_norm).strip()

    if orig_norm == rec_norm:
        lines_diff = len(lines_orig) - len(lines_rec)
        print("‚úÖ [LEVEL 2] –¢–µ–∫—Å—Ç –∑–±—ñ–≥–∞—î—Ç—å—Å—è –ø—Ä–∏ —ñ–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—ñ –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤ —Ä—è–¥–∫—ñ–≤.")
        print(f"   üìâ –í –æ—Ä–∏–≥—ñ–Ω–∞–ª—ñ: {len(lines_orig)} —Ä—è–¥–∫—ñ–≤")
        print(f"   üìà –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ:  {len(lines_rec)} —Ä—è–¥–∫—ñ–≤")

        if lines_diff > 0:
            print(f"   ‚úÇÔ∏è  –°–∫–ª–µ—î–Ω–æ (–≤—Ç—Ä–∞—á–µ–Ω–æ) –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤: {lines_diff}")
        elif lines_diff < 0:
            print(f"   ‚ûï –î–æ–¥–∞–Ω–æ –∑–∞–π–≤–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤: {abs(lines_diff)}")

        if ignore_flags: return
    else:
        print("‚ùå [LEVEL 2] –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤ —Ä—è–¥–∫—ñ–≤ –Ω–µ –ø—Ä–æ–π—à–ª–∞.")

        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è Level 2: –ø–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—É —Ä–æ–∑–±—ñ–∂–Ω—ñ—Å—Ç—å
        limit_len = min(len(orig_norm), len(rec_norm))
        diff_idx = -1
        for i in range(limit_len):
            if orig_norm[i] != rec_norm[i]:
                diff_idx = i
                break

        if diff_idx != -1:
            print(f"   üîç –ü–µ—Ä—à–∞ —Ä–æ–∑–±—ñ–∂–Ω—ñ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –Ω–∞ –ø–æ–∑–∏—Ü—ñ—ó {diff_idx}:")
            start_show = max(0, diff_idx - 20)
            end_show = min(limit_len, diff_idx + 20)

            # –ë–µ–∑–ø–µ—á–Ω–∏–π –≤–∏–≤—ñ–¥ (–∑–∞–º—ñ–Ω–∞ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ñ–≤ –Ω–∞ –∫—Ä–∞–ø–∫—É)
            safe_orig = orig_norm[start_show:end_show].replace('\n', ' ')
            safe_rec = rec_norm[start_show:end_show].replace('\n', ' ')

            print(f"   Orig: ...{safe_orig}...")
            print(f"   Rec:  ...{safe_rec}...")
            print(f"            {' ' * (diff_idx - start_show)}^")

    # --- –†–Ü–í–ï–ù–¨ 3: Skeleton ---
    # –í–∏–¥–∞–ª—è—î–º–æ –í–ó–ê–ì–ê–õ–Ü –≤—Å—ñ whitespace. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç—ñ–ª—å–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç—É (–±—É–∫–≤–∏, —Ü–∏—Ñ—Ä–∏).
    skeleton_orig = "".join(content_orig.split())
    skeleton_rec = "".join(content_rec.split())

    if skeleton_orig == skeleton_rec:
        print("‚úÖ [LEVEL 3] '–°–∫–µ–ª–µ—Ç' —Ç–µ–∫—Å—Ç—É –∑–±—ñ–≥–∞—î—Ç—å—Å—è (–¥–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ, —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –≤—Ç—Ä–∞—á–µ–Ω–æ).")
        if ignore_flags: return
    else:
        print("‚ùå [LEVEL 3] –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ '—Å–∫–µ–ª–µ—Ç—É' –Ω–µ –ø—Ä–æ–π—à–ª–∞. –í—Ç—Ä–∞—á–µ–Ω–æ –∞–±–æ –∑–º—ñ–Ω–µ–Ω–æ –¥–∞–Ω—ñ!")

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è Diff
    print(f"\n{'‚îÄ' * SEPARATOR_WIDTH}")
    print("üìù DIFF (–î–µ—Ç–∞–ª—ñ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π)")
    print(f"{'‚îÄ' * SEPARATOR_WIDTH}")

    diff = difflib.unified_diff(
        lines_orig,
        lines_rec,
        fromfile='Original',
        tofile='Recovered',
        lineterm=''
    )

    diff_lines = list(diff)

    if not diff_lines:
        print("   (Difflib –Ω–µ –∑–º—ñ–≥ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é)")
    else:
        for i, line in enumerate(diff_lines):
            if i >= DIFF_LINE_LIMIT:
                print(f"\n... —â–µ {len(diff_lines) - DIFF_LINE_LIMIT} —Ä—è–¥–∫—ñ–≤ –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ ...")
                break
            if line.startswith('+'):
                print(f"\033[92m{line.rstrip()}\033[0m") # –ó–µ–ª–µ–Ω–∏–π
            elif line.startswith('-'):
                print(f"\033[91m{line.rstrip()}\033[0m") # –ß–µ—Ä–≤–æ–Ω–∏–π
            else:
                print(line.rstrip())


# ============================================================================
# MAIN (–¢–æ—á–∫–∞ –≤—Ö–æ–¥—É)
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="–£—Ç–∏–ª—ñ—Ç–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–∞–ø–ø—ñ–Ω–≥—ñ–≤ —Ç–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("files", nargs="*", help="–®–ª—è—Ö–∏ –¥–æ .json —Ñ–∞–π–ª—ñ–≤ –∞–±–æ —á–∏—Å–ª–æ (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é)")
    parser.add_argument("--ignore-whitespace", action="store_true",
                        help="–ü—Ä–∏ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –≤–≤–∞–∂–∞—Ç–∏ —É—Å–ø—ñ—Ö–æ–º, —è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è —Ç—ñ–ª—å–∫–∏ –≤ –ø—Ä–æ–±—ñ–ª–∞—Ö")

    args = parser.parse_args()

    # ----------------------------------------------------------
    # –ï–¢–ê–ü 1: –í–ï–†–ò–§–Ü–ö–ê–¶–Ü–Ø –¢–ï–ö–°–¢–£
    # –ó–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ, —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω—ñ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ñ–∞–π–ª–∏
    # ----------------------------------------------------------
    orig, rec = find_original_and_recovery()
    if orig and rec:
        verify_text_recovery(orig, rec, args.ignore_whitespace)
        print("\n\n")
    elif not orig:
        print("‚ÑπÔ∏è  input.txt –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—é —Ç–µ–∫—Å—Ç—É –ø—Ä–æ–ø—É—â–µ–Ω–æ.\n")
    elif not rec:
        print("‚ÑπÔ∏è  input_recovery_*.txt –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—é —Ç–µ–∫—Å—Ç—É –ø—Ä–æ–ø—É—â–µ–Ω–æ.\n")

    # ----------------------------------------------------------
    # –ï–¢–ê–ü 2: –ê–ù–ê–õ–Ü–ó –¢–ê –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–ê–ü–ü–Ü–ù–ì–Ü–í
    # ----------------------------------------------------------
    files_input = args.files
    latest_maps = find_latest_maps(HISTORY_SEARCH_LIMIT)

    target_file_a = None
    target_file_b = None
    mode = "single"

    if len(files_input) == 0:
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ò–ô –†–ï–ñ–ò–ú: –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π (0) –∑ –ø–µ—Ä–µ–¥–æ—Å—Ç–∞–Ω–Ω—ñ–º (1)
        if len(latest_maps) < 2:
            if latest_maps:
                diagnose_single_file(str(latest_maps[0]))
            else:
                print("‚ùå –§–∞–π–ª—ñ–≤ masking_map_*.json –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            return
        mode = "diff"
        target_file_a = latest_maps[0]
        target_file_b = latest_maps[1]

    elif len(files_input) == 1:
        arg = files_input[0]
        # –Ø–∫—â–æ –∞—Ä–≥—É–º–µ–Ω—Ç —á–∏—Å–ª–æ -> —Ü–µ –∑–º—ñ—â–µ–Ω–Ω—è –≤ —ñ—Å—Ç–æ—Ä—ñ—ó (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 2 = –ø–æ–∑–∞–º–∏–Ω—É–ª–∏–π —Ñ–∞–π–ª)
        if arg.isdigit():
            idx = int(arg)
            if len(latest_maps) <= idx:
                print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ñ–∞–π–ª—ñ–≤ —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∑–º—ñ—â–µ–Ω–Ω—è {idx}. –ó–Ω–∞–π–¥–µ–Ω–æ: {len(latest_maps)}.")
                return
            mode = "diff"
            target_file_a = latest_maps[0]
            target_file_b = latest_maps[idx]
        else:
            # –Ø–∫—â–æ –∞—Ä–≥—É–º–µ–Ω—Ç —à–ª—è—Ö -> –ø—Ä–æ—Å—Ç–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
            diagnose_single_file(arg)
            return

    elif len(files_input) >= 2:
        # –Ø–≤–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–≤–æ—Ö –≤–∫–∞–∑–∞–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
        mode = "diff"
        target_file_a = Path(files_input[0])
        target_file_b = Path(files_input[1])

    if mode == "diff":
        print(f"üîç –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –º–∞–ø–ø—ñ–Ω–≥–∏:\n A: {target_file_a}\n B: {target_file_b}")
        if not target_file_a.exists() or not target_file_b.exists():
            print("‚ùå –û–¥–∏–Ω –∑ —Ñ–∞–π–ª—ñ–≤ –º–∞–ø–ø—ñ–Ω–≥—É –Ω–µ —ñ—Å–Ω—É—î.")
            return
        compare_mappings(target_file_a, target_file_b)

if __name__ == "__main__":
    main()
